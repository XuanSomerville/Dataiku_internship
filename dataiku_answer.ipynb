{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "import pickle_utils as pu\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_PATH = './us_census_full/census_income_learn.csv'\n",
    "TEST_PATH = './us_census_full/census_income_test.csv'\n",
    "\n",
    "# set hard coded information\n",
    "con_i = [0,5,16,17,18,30,39]    # indices of continous features\n",
    "non_i = list(set(np.arange(42))-set(con_i)) # indices of non-continous (discrete) features\n",
    "dtype = dict(it.chain(\n",
    "      zip(con_i, it.repeat(np.float32)),\n",
    "      zip(non_i, it.repeat('category'))))\n",
    "headers = [\"age\",\"class of worker\",\"detailed industry recode\",\n",
    "         \"detailed occupation recode\",\"education\",\n",
    "         \"wage per hour\",\"enroll in edu inst last wk\",\n",
    "         \"major industry code\",\"major occupation code\",\"marital stat\",\"race\",\n",
    "         \"hispanic origin\",\"sex\",\"member of a labor union\",\"reason for unemployment\",\n",
    "         \"full or part time employment stat\",\"capital gains\",\n",
    "         \"capital losses\",\"dividends from stocks\",\"tax filer stat\",\n",
    "         \"region of previous residence\",\"state of previous residence\",\n",
    "         \"detailed household and family stat\",\"detailed household summary in household\",\n",
    "         \"instance weight\",\"migration code-change in msa\",\n",
    "         \"migration code-change in reg\",\"migration code-move within reg\",\n",
    "         \"live in this house 1 year ago\",\"migration prev res in sunbelt\",\n",
    "         \"num persons worked for employer\",\"family members under 18\",\n",
    "         \"country of birth father\",\n",
    "         \"country of birth mother\",\"country of birth self\",\n",
    "         \"citizenship\",\"own business or self employed\",\n",
    "         \"fill inc questionnaire for veteran's admin\",\n",
    "         \"veterans benefits\",\"weeks worked in year\",\"year\",\"label\"]\n",
    "\n",
    "# read data\n",
    "# note: the missing value is represented by \" ?\"\n",
    "df_train = pd.read_csv(TRAIN_PATH, dtype=dtype, header=None,\n",
    "               names=headers, na_values=\" ?\")\n",
    "df_test = pd.read_csv(TEST_PATH, dtype=dtype, header=None,\n",
    "               names=headers, na_values=\" ?\")\n",
    "df = pd.concat([df_train,df_test])\n",
    "\n",
    "# make sure all the discrete features are of type \"category\"\n",
    "for i in non_i:\n",
    "    df.iloc[:,i] = df.iloc[:,i].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "determine the position of the feature \"instance weight\" by comparing the result of the following function and the \"distinct values\" provided in the file \"census_income_metadata.txt\". The reason for doing this is that this feature should not be used when training/testing classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this function takes a dataframe as input and output:\n",
    "#     dis_sizes (a list): number of distinct values of each feature\n",
    "#     dis_values (a list of list): distinct values of each feature\n",
    "def numberDistinctValues(df):\n",
    "    dis_values = []\n",
    "    dis_sizes = []\n",
    "    for i in range(df.shape[1]):\n",
    "        temp = list(df.iloc[:,i].unique())\n",
    "        dis_values.append(temp)\n",
    "        dis_sizes.append(len(temp))\n",
    "    return dis_sizes, dis_values\n",
    "\n",
    "dis_sizes, _ = numberDistinctValues(df_train)\n",
    "print(dis_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easy to see that the \"instance weight\" is the 25th feature in the dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we delete duplicated samples (rows) in the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: statistic based and univariate audit of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we plot histograms to show distribution of both numerical and categorical features.\n",
    "Warning: it takes a relative long time to output all the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getCategoricalIndices(df):\n",
    "    output = []\n",
    "    for i in range(df.shape[1]-1):\n",
    "        if(df.dtypes[i].name=='category'):\n",
    "            output.append(i)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plotContFeature(df, i):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_xlabel(\"bins\")\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "    ax.set_title(\"distribution of feature: {:s} (numerical)\".format(df.columns.values[i]))\n",
    "    df.iloc[:,i].plot.hist()\n",
    "\n",
    "def plotCateFeature(df, i):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "    ax.set_title(\"distribution of feature: {:s} (categorical)\".format(df.columns.values[i]))\n",
    "    df.iloc[:,i].value_counts().plot(kind='bar')\n",
    "\n",
    "for i in con_i:\n",
    "    plotContFeature(df, i)\n",
    "    \n",
    "for i in getCategoricalIndices(df):\n",
    "    plotCateFeature(df, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we plot boxplots for numerical features to show the quantiles and extreme values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def boxplotContFeatures(df, i):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.boxplot(df.iloc[:,i], showfliers=False)\n",
    "    ax.set_title(\"box plot of feature {} (numerical)\".format(df.columns.values[i]))\n",
    "    ax.scatter([1, 1], [df.iloc[:,i].min(), df.iloc[:,i].max()])\n",
    "print(\"In the following boxplots, extreme values are shown as blue dots, 25% and 75% \\\n",
    "quantile are shown as a rectangle, the median value is shown as a orange line\")\n",
    "for i in con_i:\n",
    "    boxplotContFeatures(df, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use \"describe\" function to obtain the above information along with other information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_descript = df.describe(include=[np.number]) # gives mean, standard deviation, mean,\n",
    "                                 # min, 25%, 50%, 75% percentile, max,\n",
    "cat_descript = df.describe(include=['category']) # gives number of distinct values,\n",
    "                                  # most frequent category and it frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we print the missing rates of the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# return missing rate of a dataframe\n",
    "def missing_rate(df):\n",
    "    return np.sum(pd.isnull(df).values,axis=0)/df.shape[0]\n",
    "\n",
    "print(\"only features with missing values are displayed\")\n",
    "print(\"numerical features' missing rate:\")\n",
    "missing_num = missing_rate(df.iloc[:, con_i])\n",
    "for i in range(len(con_i)):\n",
    "    if(missing_num[i]!=0):\n",
    "        print(\"feature #{}, {} : {:.3f}\".format(con_i[i],\n",
    "            df.columns.values[con_i[i]], missing_num[i]))\n",
    "\n",
    "print(\"categorical features' missing rate:\")\n",
    "missing_cat = missing_rate(df.iloc[:,non_i])\n",
    "for i in range(len(non_i)):\n",
    "    if(missing_cat[i]!=0):\n",
    "        print(\"feature #{}, {} : {:.3f}\".format(non_i[i],\n",
    "            df.columns.values[non_i[i]], missing_cat[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2 data preprocssing and model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the features that have high missing rates (>= 40%), we simply eliminate them. Then, we  ignore samples that have missing features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "high_missing_indices = np.nonzero(missing_rate(df)>0.4)[0]\n",
    "# eliminate features with high missing rate and the feature \"istance_weight\"\n",
    "new_indices1 = list(set(range(len(headers)))-set(list(high_missing_indices)+[24]))\n",
    "df1 = df.iloc[:,new_indices1]\n",
    "# eliminate samples with at least one missing value\n",
    "df2 = df1.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We firstly choose to test Naive Bayes Classifier (NBC) for the following reasons:\n",
    "    1. It can handle categorical features and numerical features at the same time.\n",
    "    2. It can handle categorical features with many levels in a way that does not depend on how we encode the categories. This advantage is particular important in our case, since we have some variables that have many categories.\n",
    "\n",
    "Note: since there is no available NBC implementation for mixed dataset (categorical & numerical), I implement it with a similar interface with other classifiers in sklearn. Please see the file \"naive_bayes_classifier.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let us introduce some useful functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode the categorical features\n",
    "def encodeCateFeatures(df):\n",
    "    df_encoded = df.copy()\n",
    "    for i in getCategoricalIndices(df):\n",
    "        df_encoded.iloc[:,i] = df.iloc[:,i].cat.codes.astype(\"category\")\n",
    "    return df_encoded\n",
    "\n",
    "# train-test set split\n",
    "def split_dataset(df, ratio):\n",
    "    N = df.shape[0]\n",
    "    N_train = int(ratio*N)\n",
    "    data = df.values[:,:-1].astype(np.float64)\n",
    "    label = df.values[:,-1]\n",
    "    return ((data[:N_train,:],label[:N_train]),\n",
    "              (data[N_train:,:],label[N_train:]))\n",
    "\n",
    "# train a model on training set and return its mean accuracy on validation/test set \n",
    "def evaluateModel(X_train, y_train, X_test, y_test, estimator):\n",
    "    estimator.fit(X_train, y_train)\n",
    "    return estimator.score(X_test, y_test)\n",
    "\n",
    "# 5-fold cross validation on a set for model selection\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "def CV_Selection(X, y, estimator, n_folds=5):\n",
    "    cv = StratifiedKFold(n_splits=n_folds)\n",
    "    scores = []\n",
    "    for train_index, valid_index in cv.split(X, y):\n",
    "        X_train, X_valid = X[train_index], X[valid_index]\n",
    "        y_train, y_valid = y[train_index], y[valid_index]\n",
    "        s = evaluateModel(X_train, y_train, X_valid, y_valid, estimator)\n",
    "        scores.append(s)\n",
    "    mean_score = np.mean(scores)\n",
    "    print(\"The {}-fold cross validated accuracy is {:.4f}\".format(n_folds,\n",
    "                                                            mean_score))\n",
    "    return mean_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of NBC model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getCategoricalValues(df):\n",
    "    cate_indices = getCategoricalIndices(df)\n",
    "    values = []\n",
    "    _, dis_values = numberDistinctValues(df.iloc[:,cate_indices])\n",
    "    return dis_values\n",
    "\n",
    "# encode categorical features with integers\n",
    "df3 = encodeCateFeatures(df2)\n",
    "# split the dataset into a set for training and validation, and a set for testing\n",
    "(X_train, y_train),(X_test, y_test) = split_dataset(df3, 2/3)\n",
    "# build NBC classifier\n",
    "from naive_bayes_classifier import NaiveBayesClassifier\n",
    "new_cate_indices = getCategoricalIndices(df2)\n",
    "cate_classes = getCategoricalValues(df3)\n",
    "nbc = NaiveBayesClassifier(df3.shape[1]-1, new_cate_indices, cate_classes)\n",
    "# perform stratified 5-fold cross validation on the training validation set\n",
    "CV_Selection(X_train, y_train, nbc, n_folds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result: mean accuracy of 5-fold CV on training/validation set of NBC is 0.8178"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is very bad considering that simply predicting all the samples being of class \"under 50000\" can achieve accuracy of 0.9367. This bad result is due to that NBC's condiontional indpendence assumption is too strong. In other words, the correlation between features is important for the prediction. As a result, the next model we test is logistic regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For logistic regression, how we enocde categorical features has potentially a large effect on the prediction performance. One hot encoding is a reasonable approach, but it will result in a prohibitive number of features in our case. This difficulty can be easily solved if we group together categories that are very little represented (for example, categories that have less than 0.5% of samples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of Logistic Regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# grouping categories that have fraction of samples under a specified threshold\n",
    "# The grouped category is called \"Rare Cases\"\n",
    "def groupRareCateCases(df, threshold=0.002):\n",
    "    df_grouped = df.copy()\n",
    "    for i in getCategoricalIndices(df):\n",
    "        temp = df.iloc[:,i].value_counts()\n",
    "        to_replace = temp.loc[temp<threshold*df.shape[0]].index\n",
    "        if(to_replace.size>0):\n",
    "            df_grouped.iloc[:,i] = df.iloc[:,i].replace(to_replace,\n",
    "                  ' Rare cases').astype(\"category\")\n",
    "    return df_grouped\n",
    "\n",
    "# reorder dataframe to make sure the \"label\" is at the last column\n",
    "def putLabelToLastColumn(df):\n",
    "    cols = df.columns.tolist()\n",
    "    i = 0\n",
    "    while(True):\n",
    "        if(cols[i]==\"label\"):\n",
    "            break\n",
    "        i += 1\n",
    "    cols_reordered = cols[:i] + cols[(i+1):] + [cols[i]]\n",
    "    return df[cols_reordered]\n",
    "\n",
    "df2_grouped = groupRareCateCases(df2, 0.002)\n",
    "cols = df2_grouped.columns.values[getCategoricalIndices(df2_grouped)]\n",
    "df3_onehot = pd.get_dummies(df2_grouped, columns=cols, drop_first=True)\n",
    "df3_onehot = putLabelToLastColumn(df3_onehot)\n",
    "\n",
    "(X_train, y_train),(X_test, y_test) = split_dataset(df3_onehot, 2/3)\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "lr = LR()\n",
    "CV_Selection(X_train, y_train, lr, n_folds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result: mean accuracy of 5-fold CV on training/validation set of Logistic Regression (categories under 0.2% representation rate are grouped together) is 0.9525\n",
    "\n",
    "We also show the results with other parameters:\n",
    "\n",
    "Not one-hot encoding (40 features):                                      0.9457\n",
    "\n",
    "One-hot encoding + Categories grouping (threshold=5%, 72 features):      0.9488\n",
    "\n",
    "One-hot encoding + Categories grouping (threshold=4%, 83 features):      0.9496\n",
    "\n",
    "One-hot encoding + Categories grouping (threshold=3%, 98 features):      0.9500\n",
    "\n",
    "One-hot encoding + Categories grouping (threshold=2%, 120 features):     0.9512\n",
    "\n",
    "One-hot encoding + Categories grouping (threshold=1%, 225 features):     0.9521\n",
    "\n",
    "One-hot encoding + Categories grouping (threshold=0.5%, 314 features):   0.9523\n",
    "\n",
    "One-hot encoding + Categories grouping (threshold=0.2%, 368 features):   0.9525\n",
    "\n",
    "One-hot encoding + Categories grouping (threshold=0.1%, 434 features):   0.9525"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we evaluate the best model on the test set. The best model (according to cross-validation on traning/validation set) is using one-hot encoding and categories grouping (threshold=0.2%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluateModel(X_train, y_train, X_test, y_test, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result: the mean accuracy on the test set of the selected model is 0.9526"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to obtain a clear insight on the most predictive features for this classification problem, we perform feature selection using L1 penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# when performing feature selection, we don't use one-hot encoding\n",
    "df3 = encodeCateFeatures(df2)\n",
    "(X_train, y_train),(X_test, y_test) = split_dataset(df3, 2/3)\n",
    "lr_l1 = LR(C=0.0001, penalty=\"l1\").fit(X_train, y_train)\n",
    "coeff_l1 = np.squeeze(lr_l1.coef_)\n",
    "selected_features = np.nonzero(coeff_l1!=0)[0]\n",
    "df_base = df3\n",
    "print(df_base.columns[selected_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the feature selection using the best model that we selected previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# select features\n",
    "df2_selected = pd.concat([df3.iloc[:,selected_features], \n",
    "                          df3.iloc[:,-1]], axis=1)\n",
    "# grouping rare categories and using one-hot encoding\n",
    "df2_selected_grouped = groupRareCateCases(df2_selected, 0.005)\n",
    "cols = df2_selected_grouped.columns.values[getCategoricalIndices(df2_selected_grouped)]\n",
    "df3_selected_onehot = pd.get_dummies(df2_selected_grouped, columns=cols, drop_first=True)\n",
    "df3_selected_onehot = putLabelToLastColumn(df3_selected_onehot)\n",
    "# train-test set split\n",
    "(X_train, y_train),(X_test, y_test) = split_dataset(df3_selected_onehot, 2/3)\n",
    "# train model on training set and evaluate it on test set\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "lr = LR()\n",
    "evaluateModel(X_train, y_train, X_test, y_test, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result:\n",
    "\n",
    "When C=10^(-4) (inverse regularization strength), the mean accuracy is 0.9467, the selected features are: 'age', 'detailed occupation recode', 'wage per hour', 'major occupation code', 'capital gains', 'capital losses', 'dividends from stocks', 'tax filer stat', 'state of previous residence', 'detailed household and family stat', 'country of birth father', 'country of birth mother', 'country of birth self', 'weeks worked in year'\n",
    "\n",
    "When C=10^(-5), the mean accuracy is 0.9439, the selected features are:\n",
    "'wage per hour', 'capital gains', 'capital losses', 'dividends from stocks', 'state of previous residence', 'country of birth self', 'weeks worked in year'.\n",
    "\n",
    "When C=5*10^(-7), the mean accuracy is 0.9427, the selected features are: 'wage per hour', 'capital gains', 'dividends from stocks', 'country of birth self'\n",
    "\n",
    "When C=2*10^(-7), the mean accuracy is 0.9426, the selected features are: 'wage per hour', 'capital gains'\n",
    "\n",
    "When C=10^(-7), the mean accuracy is 0.9422, the selected features are: 'capital gains'\n",
    "\n",
    "In conclusion, the most predictive features are 'capital gains' and 'wage per hour'."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
